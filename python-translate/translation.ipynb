{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "translator.raise_Exception = True\n",
    "\n",
    "# the text may be too long, so we will break it into chunks\n",
    "\n",
    "def get_chunks(s, maxlength, separator):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while start + maxlength  < len(s) and end != -1:\n",
    "        end = s.rfind(separator, start, start + maxlength + 1)\n",
    "        yield s[start:end]\n",
    "        start = end +1\n",
    "    yield s[start:]\n",
    "\n",
    "output = \"\"\n",
    "\n",
    "def translate_doc(filename):\n",
    "    global df\n",
    "    path = 'data/' + filename\n",
    "    print(path)\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        # count of characters\n",
    "        number_of_characters = 0\n",
    "        # new list to get rid of \\n\n",
    "        remove_n = []\n",
    "\n",
    "        for line in lines:\n",
    "                number_of_characters = number_of_characters + len(line)\n",
    "                remove_n.append(line.strip())\n",
    "\n",
    "        joined_lines = ''.join(remove_n)\n",
    "        global output\n",
    "\n",
    "        \n",
    "        if number_of_characters > 5000:\n",
    "            \n",
    "            for group in get_chunks(joined_lines, 5000, \".\"):\n",
    "                # print(len(group))\n",
    "                if len(group) < 5000:\n",
    "                    print(\". option\", len(group))\n",
    "                    group = translator.translate(group)\n",
    "                    #Make list with line lengths:\n",
    "                    # [(n, len(n)) for n in chunks]\n",
    "                    output = output + group.text\n",
    "                else:\n",
    "                    for group in get_chunks(joined_lines, 5000, \";\"):\n",
    "                        \n",
    "                        if len(group) < 5000:\n",
    "                            print(\"; option\", len(group))\n",
    "                            group = translator.translate(group)\n",
    "                            #Make list with line lengths:\n",
    "                            # [(n, len(n)) for n in chunks]\n",
    "                            output = output + group.text\n",
    "                        else:\n",
    "                            for group in get_chunks(joined_lines, 5000, \",\"):\n",
    "                                # print(len(group))\n",
    "                                if len(group) < 5000:\n",
    "                                    print(\", option\", len(group))\n",
    "                                    group = translator.translate(group)\n",
    "                                    #Make list with line lengths:\n",
    "                                    # [(n, len(n)) for n in chunks]\n",
    "                                    output = output + group.text\n",
    "                    \n",
    "                \n",
    "        else:\n",
    "            print(\"shorter than 5000s\")\n",
    "            output = translator.translate(joined_lines)\n",
    "            output = output.text\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# iterate through the folder\n",
    "\n",
    "def translate_all(directory):\n",
    "    # create       \n",
    "    data = {'File': [],\n",
    "        'Translation': []}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # print(filename)\n",
    "            translate_doc(filename)\n",
    "            values_to_add = {'File': filename, 'Translation': output}\n",
    "            row_to_add = pd.Series(values_to_add, name=filename)\n",
    "            # print(row_to_add)\n",
    "\n",
    "            df = df.append(row_to_add)\n",
    "            # print(df.shape)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    time.sleep(2)\n",
    "    return df\n",
    "\n",
    "directory = 'data/'\n",
    "df = translate_all(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/translated.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "translator.raise_Exception = True\n",
    "\n",
    "# the text may be too long, so we will break it into chunks\n",
    "\n",
    "def get_chunks(s, maxlength, separator):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while start + maxlength  < len(s) and end != -1:\n",
    "        end = s.rfind(separator, start, start + maxlength + 1)\n",
    "        yield s[start:end]\n",
    "        start = end +1\n",
    "    yield s[start:]\n",
    "\n",
    "output = \"\"\n",
    "\n",
    "def translate_doc(filename):\n",
    "    global df\n",
    "    path = 'data/' + filename\n",
    "    print(path)\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        # count of characters\n",
    "        number_of_characters = 0\n",
    "        # new list to get rid of \\n\n",
    "        remove_n = []\n",
    "\n",
    "        for line in lines:\n",
    "                number_of_characters = number_of_characters + len(line)\n",
    "                remove_n.append(line.strip())\n",
    "\n",
    "        joined_lines = ''.join(remove_n)\n",
    "        global output\n",
    "\n",
    "        \n",
    "        if number_of_characters > 5000:\n",
    "            \n",
    "            for group in get_chunks(joined_lines, 5000, \".\"):\n",
    "                # print(len(group))\n",
    "                if len(group) < 5000:\n",
    "                    print(\". option\", len(group))\n",
    "                    group = translator.translate(group)\n",
    "                    #Make list with line lengths:\n",
    "                    # [(n, len(n)) for n in chunks]\n",
    "                    output = output + group.text\n",
    "                else:\n",
    "                    for group in get_chunks(joined_lines, 5000, \";\"):\n",
    "                        \n",
    "                        if len(group) < 5000:\n",
    "                            print(\"; option\", len(group))\n",
    "                            group = translator.translate(group)\n",
    "                            #Make list with line lengths:\n",
    "                            # [(n, len(n)) for n in chunks]\n",
    "                            output = output + group.text\n",
    "                        else:\n",
    "                            for group in get_chunks(joined_lines, 5000, \",\"):\n",
    "                                # print(len(group))\n",
    "                                if len(group) < 5000:\n",
    "                                    print(\", option\", len(group))\n",
    "                                    group = translator.translate(group)\n",
    "                                    #Make list with line lengths:\n",
    "                                    # [(n, len(n)) for n in chunks]\n",
    "                                    output = output + group.text\n",
    "                    \n",
    "                \n",
    "        else:\n",
    "            print(\"shorter than 5000s\")\n",
    "            output = translator.translate(joined_lines)\n",
    "            output = output.text\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# iterate through the folder\n",
    "\n",
    "def translate_all(directory):\n",
    "    # create       \n",
    "    data = {'File': [],\n",
    "        'Translation': []}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # print(filename)\n",
    "            translate_doc(filename)\n",
    "            values_to_add = {'File': filename, 'Translation': output}\n",
    "            row_to_add = pd.Series(values_to_add, name=filename)\n",
    "            # print(row_to_add)\n",
    "\n",
    "            df = df.append(row_to_add)\n",
    "            # print(df.shape)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    time.sleep(2)\n",
    "    return df\n",
    "\n",
    "directory = 'data/'\n",
    "df = translate_all(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.39k/1.39k [00:00<00:00, 285kB/s]\n",
      "Downloading: 100%|██████████| 2.28G/2.28G [08:25<00:00, 4.83MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import goslate\n",
    "gs = goslate.Goslate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27196\n"
     ]
    }
   ],
   "source": [
    "number_of_characters = 0\n",
    "remove_n = []\n",
    "\n",
    "with open(\"../data/2019041722.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "                number_of_characters = number_of_characters + len(line)\n",
    "                remove_n.append(line.strip())\n",
    "\n",
    "joined_lines = ''.join(remove_n)\n",
    "# print(joined_lines)\n",
    "print(number_of_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_characters = 0\n",
    "\n",
    "with open(\"../data/2019041722.txt\", 'r') as f:    \n",
    "        lines = f.read()\n",
    "        lines = lines.split('\\n',1)[-1] # remove first line\n",
    "        number_of_characters = len(lines)\n",
    "        lines = lines.replace(\".\\n\", \"**/m\" # keep par breaks\n",
    "                      ).replace(\". \\n\", \"**/m\" # keep par breaks\n",
    "                      ).replace(\"\\n\", \"\" # delete in-par breaks     \n",
    "                      ).replace(\"**/m\", \".\\n\\n\" # restore par break\n",
    "                      ).replace(\"ы\", \"ë\")\n",
    "        print(number_of_characters)\n",
    "        # split paragraphs\n",
    "        paragraphs = lines.split(\".\\n\\n\")\n",
    "        print(type(paragraphs))\n",
    "        for paragraph in paragraphs:\n",
    "                print(len(paragraph), \"par_lenght\")\n",
    "                if len(paragraph) > 5000:\n",
    "                        paragraphs1 = lines.split(\". \\n\")\n",
    "                        for paragraph1 in paragraphs1:\n",
    "                                paragraphs.append(paragraph1)\n",
    "                                print(\"split_par\", paragraph1, len(paragraph1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_extracted(text):\n",
    "  \"\"\"Wrapper for Google Translate with upload workaround.\"\"\"\n",
    "  # Set-up and wrap translation client\n",
    "  translate = GoogleTranslator(source='auto', target='en').translate\n",
    "  # Split input text into a list of sentences\n",
    "  sentences = sent_tokenize(text)\n",
    "  # Initialize containers\n",
    "  translated_text = ''\n",
    "  source_text_chunk = ''\n",
    "\n",
    "  # collect chuncks of sentences, translate individually\n",
    "  for sentence in sentences:\n",
    "    # if chunck + current sentence < limit, add the sentence\n",
    "    if ((len(sentence.encode('utf-8')) +  len(source_text_chunk.encode('utf-8')) < 5000)):\n",
    "      source_text_chunk += ' ' + sentence\n",
    "    # else translate chunck and start new one with current sentence\n",
    "    else:\n",
    "      translated_text += ' ' + translate(source_text_chunk)\n",
    "\n",
    "     # if current sentence smaller than 5000 chars, start new chunck\n",
    "    if (len(sentence.encode('utf-8')) < 5000):\n",
    "       source_text_chunk = sentence\n",
    "     # else, replace sentence with notification message\n",
    "    else:\n",
    "       message = \"<<Omitted Word longer than 5000bytes>>\"\n",
    "       translated_text += ' ' + translate(message)\n",
    "       # Re-set text container to empty\n",
    "       source_text_chunk = ''\n",
    "\n",
    "  # Translate the final chunk of input text, if there is any valid   text left to translate\n",
    "  if translate(source_text_chunk) != None:\n",
    "    translated_text += ' ' + translate(source_text_chunk)\n",
    "  return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotValidPayload",
     "evalue": "6 --> text must be a valid text with maximum 5000 character, otherwise it cannot be translated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotValidPayload\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24180/3975448995.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m         translated = '.\\n\\n'.join(\n\u001b[1;32m----> 3\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mtranslate_extracted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparagraphs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m                 )\n\u001b[0;32m      5\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24180/3975448995.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m         translated = '.\\n\\n'.join(\n\u001b[1;32m----> 3\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mtranslate_extracted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparagraphs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m                 )\n\u001b[0;32m      5\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24180/2597170000.py\u001b[0m in \u001b[0;36mtranslate_extracted\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m   \u001b[1;31m# Translate the final chunk of input text, if there is any valid   text left to translate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_text_chunk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mtranslated_text\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_text_chunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtranslated_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Irina\\anaconda3\\lib\\site-packages\\deep_translator\\google.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtranslated\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \"\"\"\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mis_input_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_same_source_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Irina\\anaconda3\\lib\\site-packages\\deep_translator\\validate.py\u001b[0m in \u001b[0;36mis_input_valid\u001b[1;34m(text, min_chars, max_chars)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotValidPayload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmin_chars\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_chars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotValidLength\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotValidPayload\u001b[0m: 6 --> text must be a valid text with maximum 5000 character, otherwise it cannot be translated"
     ]
    }
   ],
   "source": [
    "if lines != '':\n",
    "        translated = '.\\n\\n'.join(\n",
    "                [translate_extracted(paragraph) for paragraph in paragraphs]\n",
    "                )\n",
    "else:\n",
    "    translated = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24180/2024688032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'translated' is not defined"
     ]
    }
   ],
   "source": [
    "print(translated)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "573c5f8e4a9de4a29cc2c43f44bd7d1acbfa6c8b3e00d78238142f0788ca6380"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
